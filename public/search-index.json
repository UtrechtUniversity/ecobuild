[{"title":"Platform Development","bio":"","description":"The team went to the seaside for a weekend of team building and brainstorming.","name":"","role":"","authors":"","content":" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. ","url":"/activities/development"},{"title":"Platform Improvement and Scaling","bio":"","description":"We are continually improving our platform.","name":"","role":"","authors":"","content":" The lab organized a summer school for over 50 undergraduate students. Topics included introduction to PyTorch, CNNs, and Transformers.","url":"/activities/improvements"},{"title":"Focus Groups, Surveys and Interviews","bio":"","description":"Prof. John Doe held a focus group to assert things.","name":"","role":"","authors":"","content":" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. ","url":"/activities/interviews"},{"title":"Platform Testing","bio":"","description":"We hosted a 3-day workshop on Deep Learning for undergraduates.","name":"","role":"","authors":"","content":" The lab organized a summer school for over 50 undergraduate students. Topics included introduction to PyTorch, CNNs, and Transformers. ","url":"/activities/testing"},{"title":"Pattern Recognition and Machine Learning","bio":"","description":"This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible.","name":"","role":"","authors":"[\"Christopher M. Bishop\"]","content":"This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible.","url":"/books/2006-Bishop-pattern-recognition-and-machin"},{"title":"Computer Vision: Algorithms and Applications","bio":"","description":"This book explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching.","name":"","role":"","authors":"[\"Richard Szeliski\"]","content":"This book explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching.","url":"/books/2010-Szeliski-computer-vision-algorithms-and"},{"title":"Deep Learning","bio":"","description":"An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.","name":"","role":"","authors":"[\"Ian Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"]","content":"An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.","url":"/books/2016-Goodfellow-deep-learning"},{"title":"Reinforcement Learning: An Introduction","bio":"","description":"The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.","name":"","role":"","authors":"[\"Richard S. Sutton\", \"Andrew G. Barto\"]","content":"The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.","url":"/books/2018-Sutton-reinforcement-learning-an-intr"},{"title":"18th Challenge Cup National College Student Extracurricular Academic Science and Technology Works Competition","bio":"","description":"","name":"","role":"","authors":"","content":"","url":"/honors/challenge-cup"},{"title":"Second Prize of National Mathematical Modeling Contest","bio":"","description":"","name":"","role":"","authors":"","content":"Awarded for the project \"Optimization of Logistics Network\". ","url":"/honors/honor-second"},{"title":"Special Award of Internet+","bio":"","description":"","name":"","role":"","authors":"","content":"Awarded for the project \"Next Gen AI\". ","url":"/honors/honor-special"},{"title":"Third Prize of ACM-ICPC Regional","bio":"","description":"","name":"","role":"","authors":"","content":"Awarded to the team \"CodeWarriors\". ","url":"/honors/honor-third"},{"title":"Lab Awarded $1M NSF Grant","bio":"","description":"","name":"","role":"","authors":"","content":"We are thrilled to announce that the Scholar-Lite Lab has been awarded a $1M grant from the NSF. This funding will support our research on making computer vision systems more robust to adversarial attacks and distribution shifts over the next three years. ","url":"/news/2025-grant"},{"title":"Best Paper Award at CVPR 2025","bio":"","description":"","name":"","role":"","authors":"","content":"We are incredibly honored to receive the Best Paper Award at CVPR 2025 for our work on \"Efficient Vision Transformers with Sparse Attention Mechanisms\". Congratulations to the authors Alex Morgan, Sarah Chen, and David Kim! ","url":"/news/2025-paper-award"},{"title":"One paper accepted to CVPR 2026","bio":"","description":"","name":"","role":"","authors":"","content":"We are excited to share that our latest research has been accepted for publication in the **Conference on Computer Vision and Pattern Recognition (CVPR) 2026**.  The paper titled \"A Novel Approach...\" introduces a groundbreaking method for... ","url":"/news/cvpr-acceptance"},{"title":"A Method for Image Recognition Based on Attention Mechanism","bio":"","description":"","name":"","role":"","authors":"","content":"","url":"/patents/attention-method"},{"title":"Method for Image Recognition","bio":"","description":"","name":"","role":"","authors":"","content":"A patent describing a new method for recognizing objects in cluttered scenes. ","url":"/patents/patent-1"},{"title":"Smart Traffic Control System","bio":"","description":"","name":"","role":"","authors":"","content":"A system for optimizing traffic flow using real-time camera data. ","url":"/patents/patent-2"},{"title":"Generative Adversarial Nets","bio":"","description":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.","name":"","role":"","authors":"[\"Ian Goodfellow\", \"Jean Pouget-Abadie\", \"Mehdi Mirza\", \"Bing Xu\", \"David Warde-Farley\", \"Sherjil Ozair\", \"Aaron Courville\", \"Yoshua Bengio\"]","content":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.","url":"/publications/2014-Goodfellow-generative-adversarial-nets"},{"title":"Deep residual learning for image recognition","bio":"","description":"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.","name":"","role":"","authors":"[\"Kaiming He\", \"Xiangyu Zhang\", \"Shaoqing Ren\", \"Jian Sun\"]","content":"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.","url":"/publications/2016-He-deep-residual-learning-for-ima"},{"title":"Attention is all you need","bio":"","description":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","name":"","role":"","authors":"[\"Ashish Vaswani\", \"Noam Shazeer\", \"Niki Parmar\", \"Jakob Uszkoreit\", \"Llion Jones\", \"Aidan N Gomez\", \"\\Lukasz Kaiser\", \"Illia Polosukhin\"]","content":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","url":"/publications/2017-Vaswani-attention-is-all-you-need"},{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","bio":"","description":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.","name":"","role":"","authors":"[\"Jacob Devlin\", \"Ming-Wei Chang\", \"Kenton Lee\", \"Kristina Toutanova\"]","content":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.","url":"/publications/2019-Devlin-bert-pre-training-of-deep-bidi"},{"title":"Understanding Context in Large Language Models","bio":"","description":"We investigate how large language models (LLMs) utilize context in long-document understanding tasks. Our analysis reveals that while models are capable of attending to distant tokens, they often rely on local heuristics for decision making.","name":"","role":"","authors":"[\"Sarah Johnson\", \"Wei Zhang\", \"Alex Smith\"]","content":"We investigate how large language models (LLMs) utilize context in long-document understanding tasks. Our analysis reveals that while models are capable of attending to distant tokens, they often rely on local heuristics for decision making.","url":"/publications/2023-Johnson-understanding-context-in-large"},{"title":"High-Fidelity Image Generation with Latent Diffusion Models","bio":"","description":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining.","name":"","role":"","authors":"[\"Emily Chen\", \"David Wang\", \"Michael Brown\"]","content":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining.","url":"/publications/2024-Chen-high-fidelity-image-generation"},{"title":"GPT-4 Technical Report","bio":"","description":"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks.","name":"","role":"","authors":"[\"OpenAI\"]","content":"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks.","url":"/publications/2024-OpenAI-gpt-4-technical-report"},{"title":"Vision Transformers for Small Datasets","bio":"","description":"Vision Transformers (ViT) typically require massive datasets to outperform CNNs. We propose a novel regularization technique and architecture modification that allows ViTs to train effectively on small datasets (e.g., CIFAR-10, Flowers-102) from scratch, achieving state-of-the-art performance.","name":"","role":"","authors":"[\"Wei Zhang\", \"Jian Li\", \"Sarah Johnson\"]","content":"Vision Transformers (ViT) typically require massive datasets to outperform CNNs. We propose a novel regularization technique and architecture modification that allows ViTs to train effectively on small datasets (e.g., CIFAR-10, Flowers-102) from scratch, achieving state-of-the-art performance.","url":"/publications/2025-Zhang-vision-transformers-for-small-"},{"title":"Computer Vision","bio":"","description":"Our research in computer vision focuses on object detection, segmentation, and 3D reconstruction.","name":"","role":"","authors":"","content":"# Computer Vision Research  Our lab works on state-of-the-art computer vision algorithms.  ## Key Areas - Object Detection - Semantic Segmentation - 3D Vision  We apply these technologies to real-world problems such as autonomous driving and medical imaging. ","url":"/research/cv"},{"title":"Natural Language Processing","bio":"","description":"Investigating large language models, sentiment analysis, and human-computer interaction.","name":"","role":"","authors":"","content":"Our NLP research focuses on: - Large Language Model fine-tuning - Multimodal understanding - Dialogue systems ","url":"/research/nlp"},{"title":"Intelligent Image Processing System V1.0","bio":"","description":"An automated image processing platform based on deep learning, featuring real-time object detection and semantic segmentation capabilities.","name":"","role":"","authors":"","content":"","url":"/softwares/image-proc-sys"},{"title":"Intelligent Medical Image Analysis System","bio":"","description":"A software system for automated analysis of CT scans using deep learning.","name":"","role":"","authors":"","content":"","url":"/softwares/soft-1"},{"title":"[\"Lecturer\"]","bio":"","description":"","name":"Dr. J.H. (Jaco) Appelman","role":"Lecturer","authors":"","content":"Emily is a third-year PhD student. Her research aims to make deep learning models more transparent and interpretable. She recently interned at Google Research working on fairness metrics for large language models. ","url":"/team/dr-jaco-appelman"},{"title":"[\"Principal Investigator\", \"Researcher\"]","bio":"","description":"","name":"Dr. Katharina Hecht","role":"Principal Investigator","authors":"","content":"With my interdisciplinary research approach I aim to contribute to bridge ecological science and the building industry. ","url":"/team/dr-katharina-hecht"},{"title":"[\"Associate Professor\"]","bio":"","description":"","name":"Dr. M. (Michael) Behrisch","role":"Associate Professor","authors":"","content":" ","url":"/team/dr-michael-behrisch"},{"title":"[\"Policy Advisor\"]","bio":"","description":"","name":"Mark Borst MSc MA","role":"Policy Advisor","authors":"","content":"Alex is investigating how LLMs can better understand and generate complex software systems. He is also interested in the intersection of PL and AI. ","url":"/team/mark-borst"}]